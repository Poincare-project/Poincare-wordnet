{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2Y8XgXEvBxdw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Embedding\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S_WYCNOnB303"
      },
      "outputs": [],
      "source": [
        "class PoincareModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size, init_weights=1e-3, epsilon=1e-7):\n",
        "        \"\"\"\n",
        "        Constructeur de la classe PoincareModel.\n",
        "\n",
        "        Args:\n",
        "            dim (int): Dimension des vecteurs d'embedding.\n",
        "            size (int): Taille du dictionnaire (nombre d'entités distinctes).\n",
        "            init_weights (float): Poids d'initialisation pour les embeddings.\n",
        "            epsilon (float): Petit epsilon utilisé pour éviter les divisions par zéro.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "\n",
        "        # Crée une couche d'embedding avec des vecteurs d'embedding de dimension 'dim' pour 'size' entités.\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, sparse=False)\n",
        "\n",
        "        # Initialise les poids d'embedding de manière aléatoire dans l'intervalle [-init_weights, init_weights].\n",
        "        self.embedding.weight.data.uniform_(-init_weights, init_weights)\n",
        "\n",
        "        # Stocke la valeur d'epsilon utilisée pour éviter les divisions par zéro dans la distance.\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "\n",
        "\n",
        "    def dist(self, u, v):\n",
        "        \"\"\"\n",
        "        Calcule la distance entre deux vecteurs d'embedding dans l'espace hyperbolique de Poincaré.\n",
        "\n",
        "        Args:\n",
        "            u (Tensor): Le premier vecteur d'embedding.\n",
        "            v (Tensor): Le deuxième vecteur d'embedding.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: La distance entre les deux vecteurs d'embedding.\n",
        "        \"\"\"\n",
        "        # Calcule la distance de Poincaré entre u et v.\n",
        "        sqdist = torch.sum((u - v) ** 2, dim=-1)\n",
        "        squnorm = torch.sum(u ** 2, dim=-1)\n",
        "        sqvnorm = torch.sum(v ** 2, dim=-1)\n",
        "        x = 1 + 2 * sqdist / ((1 - squnorm) * (1 - sqvnorm)) + self.epsilon\n",
        "        z = torch.sqrt(x ** 2 - 1)\n",
        "        return torch.log(x + z)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Implémentation de la passe avant (forward) du modèle.\n",
        "\n",
        "        Args:\n",
        "            inputs (Tensor): Indices des entités en entrée.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Les distances entre les embeddings des entités.\n",
        "        \"\"\"\n",
        "        # Obtient les embeddings des entités correspondant aux indices en entrée.\n",
        "        e = self.embedding(inputs)\n",
        "        \n",
        "        # Sépare la première dimension (indice 0) pour obtenir le point de départ (s) et le reste (o).\n",
        "        o = e.narrow(dim=1, start=1, length=e.size(1) - 1)\n",
        "        s = e.narrow(dim=1, start=0, length=1).expand_as(o)\n",
        "        \n",
        "        # Calcule la distance entre les deux embeddings.\n",
        "        return self.dist(s, o)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-1.8215,  2.1824,  0.8829]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "word_frequency = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "words = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
        "categorical_dist = Categorical(torch.tensor(word_frequency))\n",
        "uniform_dist = Categorical(torch.ones(len(word_frequency)) / len(word_frequency))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
